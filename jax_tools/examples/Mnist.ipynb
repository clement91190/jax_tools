{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import haiku as hk\n",
    "from itertools import cycle\n",
    "import tensorflow_datasets as tfds\n",
    "# improving result with Adam from optax\n",
    "import optax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1., 0., 0., 0.],\n",
       "             [1., 0., 0., 0.],\n",
       "             [0., 0., 0., 0.],\n",
       "             [0., 0., 1., 0.],\n",
       "             [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_hot returns vector with only one 1 for the label index.\n",
    "jax.nn.one_hot(jnp.array([0, 0, -1, 2, 2]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='/home/clement/tensorflow_datasets/mnist/3.0.1',\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the dataset directly\n",
    "mnist = tfds.image.MNIST()\n",
    "# useful to know what's inside the dataset\n",
    "mnist.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/clement/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ce1675cb8948f79e3afca0d87e289d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Completed...'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to /home/clement/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mnist.download_and_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = mnist.as_dataset(batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(logits, labels):\n",
    "    one_hot = jax.nn.one_hot(labels, logits.shape[-1])\n",
    "    return -jnp.sum(jax.nn.log_softmax(logits) * one_hot, axis=-1)\n",
    "\n",
    "\n",
    "def net_fn(images):\n",
    "    # LNET 300 100 10\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Flatten(),\n",
    "        hk.Linear(300),\n",
    "        hk.Linear(100),\n",
    "        hk.Linear(10)])\n",
    "    return mlp(images)\n",
    "\n",
    "\n",
    "# hk.Conv2D(output_channels=10, kernel_shape=6), jax.nn.relu,\n",
    "        \n",
    "\n",
    "# There are two transforms in Haiku, hk.transform and hk.transform_with_state.\n",
    "# If our network updated state during the forward pass (e.g. like the moving\n",
    "# averages in hk.BatchNorm) we would need hk.transform_with_state, but for our\n",
    "# simple MLP we can just use hk.transform.\n",
    "net_fn_t = hk.transform(net_fn)\n",
    "# MLP is deterministic once we have our parameters, as such we will not need to\n",
    "# pass an RNG key to apply. without_apply_rng is a convenience wrapper that will\n",
    "# make the rng argument to `loss_fn_t.apply` default to `None`.\n",
    "net_fn_t = hk.without_apply_rng(net_fn_t)\n",
    "\n",
    "\n",
    "def loss_fn(params, images, labels):\n",
    "    logits = net_fn_t.apply(params, images)\n",
    "    return jnp.mean(softmax_cross_entropy(logits, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def result_on_test_set(parameters):\n",
    "    # Test on test set : \n",
    "    it_test = datasets['test'].as_numpy_iterator()\n",
    "\n",
    "    def error_rate(y, y_hat):\n",
    "        return jnp.mean(y != y_hat)\n",
    "\n",
    "    errs = []\n",
    "    for o in it_test:\n",
    "        images, labels = o['image'].astype(jnp.float32) / 255., o['label']\n",
    "        label_hat = net_fn_t.apply(parameters, images).argmax(axis=1)\n",
    "        errs.append(error_rate(labels, label_hat))\n",
    "    print(\"Error on test set \", np.mean(errs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on train set  2.2960014\n",
      "Error on test set  0.8116\n",
      "Loss on train set  1.888028\n",
      "Error on test set  0.3524\n",
      "Loss on train set  1.5463176\n",
      "Error on test set  0.2642\n",
      "Loss on train set  1.2657787\n",
      "Error on test set  0.22259998\n",
      "Loss on train set  1.0318453\n",
      "Error on test set  0.2006\n",
      "Loss on train set  0.89268565\n",
      "Error on test set  0.18149999\n",
      "Loss on train set  0.7739114\n",
      "Error on test set  0.16849999\n",
      "Loss on train set  0.717366\n",
      "Error on test set  0.1593\n",
      "Loss on train set  0.6926508\n",
      "Error on test set  0.1504\n",
      "Loss on train set  0.6375088\n",
      "Error on test set  0.1418\n"
     ]
    }
   ],
   "source": [
    "# `init` runs your function, as such we need an example input. Typically you can\n",
    "# pass \"dummy\" inputs (e.g. ones of the same shape and dtype) since initialization\n",
    "# is not usually data dependent.\n",
    "\n",
    "it = cycle(datasets['train'].as_numpy_iterator())\n",
    "\n",
    "o = next(it)\n",
    "images, labels = o['image'].astype(jnp.float32) / 255., o['label']\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# The result of `init` is a nested data structure of all the parameters in your\n",
    "# network. You can pass this into `apply`.\n",
    "params = loss_fn_t.init(rng, images, labels)\n",
    "\n",
    "\n",
    "def sgd(param, update):\n",
    "    return param - 0.01 * update\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    o = next(it)\n",
    "    images, labels = o['image'].astype(jnp.float32) / 255., o['label']\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, images, labels)\n",
    "    params = jax.tree_multimap(sgd, params, grads)\n",
    "    if i % 50 == 0:\n",
    "        print(\"Loss on train set \", loss)\n",
    "        result_on_test_set(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def ema_update(\n",
    "    avg_params: hk.Params,\n",
    "    new_params: hk.Params,\n",
    "    epsilon: float = 0.001) -> hk.Params:\n",
    "    return jax.tree_multimap(lambda p1, p2: (1 - epsilon) * p1 + epsilon * p2,\n",
    "         avg_params, new_params)\n",
    "\n",
    "\n",
    "def optimize(steps=1000):\n",
    "    opt = optax.adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "    o = next(it)\n",
    "    images, labels = o['image'].astype(jnp.float32) / 255., o['label']\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    # The result of `init` is a nested data structure of all the parameters in your\n",
    "    # network. You can pass this into `apply`.\n",
    "    params = net_fn_t.init(rng, images)\n",
    "    avg_params = params\n",
    "    opt_state = opt.init(params)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(steps):\n",
    "        o = next(it)\n",
    "        images, labels = o['image'].astype(jnp.float32) / 255., o['label']\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, images, labels)\n",
    "\n",
    "        updates, opt_state = opt.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        avg_params = ema_update(avg_params, params)\n",
    "        if i % 100 == 0:\n",
    "            print(\"Loss on train set \", loss)\n",
    "            result_on_test_set(avg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on train set  2.3423822\n",
      "Error on test set  0.93509996\n",
      "Loss on train set  0.2918328\n",
      "Error on test set  0.29759997\n",
      "Loss on train set  0.2829472\n",
      "Error on test set  0.17999999\n",
      "Loss on train set  0.24500448\n",
      "Error on test set  0.147\n",
      "Loss on train set  0.27240166\n",
      "Error on test set  0.12920001\n",
      "Loss on train set  0.2591893\n",
      "Error on test set  0.118900016\n",
      "Loss on train set  0.23423465\n",
      "Error on test set  0.11029999\n",
      "Loss on train set  0.2619259\n",
      "Error on test set  0.1039\n",
      "Loss on train set  0.2514375\n",
      "Error on test set  0.099199995\n",
      "Loss on train set  0.23181894\n",
      "Error on test set  0.095\n"
     ]
    }
   ],
   "source": [
    "optimize(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on train set  2.347464\n",
      "Error on test set  0.9476\n",
      "Loss on train set  0.15585364\n",
      "Error on test set  0.1347\n",
      "Loss on train set  0.06889652\n",
      "Error on test set  0.13960001\n",
      "Loss on train set  0.05627359\n",
      "Error on test set  0.15330002\n",
      "Loss on train set  0.033857062\n",
      "Error on test set  0.1635\n",
      "Loss on train set  0.029153084\n",
      "Error on test set  0.16849999\n",
      "Loss on train set  0.065876655\n",
      "Error on test set  0.16600001\n",
      "Loss on train set  0.021700073\n",
      "Error on test set  0.1596\n",
      "Loss on train set  0.012087165\n",
      "Error on test set  0.15309998\n",
      "Loss on train set  0.01262256\n",
      "Error on test set  0.1405\n",
      "Loss on train set  0.012617512\n",
      "Error on test set  0.1232\n",
      "Loss on train set  0.027628185\n",
      "Error on test set  0.107599996\n",
      "Loss on train set  0.020550411\n",
      "Error on test set  0.093\n",
      "Loss on train set  0.00369231\n",
      "Error on test set  0.0795\n",
      "Loss on train set  0.018902548\n",
      "Error on test set  0.0685\n",
      "Loss on train set  0.01752936\n",
      "Error on test set  0.058699995\n",
      "Loss on train set  0.013117411\n",
      "Error on test set  0.052599996\n",
      "Loss on train set  0.0063632443\n",
      "Error on test set  0.0462\n",
      "Loss on train set  0.006159285\n",
      "Error on test set  0.040400006\n",
      "Loss on train set  0.0051221712\n",
      "Error on test set  0.0365\n",
      "Loss on train set  0.008930988\n",
      "Error on test set  0.0333\n",
      "Loss on train set  0.003270341\n",
      "Error on test set  0.030899998\n",
      "Loss on train set  0.0033880873\n",
      "Error on test set  0.0285\n",
      "Loss on train set  0.009671787\n",
      "Error on test set  0.027\n",
      "Loss on train set  0.0038890212\n",
      "Error on test set  0.0249\n",
      "Loss on train set  0.010108298\n",
      "Error on test set  0.0231\n",
      "Loss on train set  0.003803223\n",
      "Error on test set  0.0225\n",
      "Loss on train set  0.011971421\n",
      "Error on test set  0.022300001\n",
      "Loss on train set  0.0029592933\n",
      "Error on test set  0.021000002\n",
      "Loss on train set  0.011025079\n",
      "Error on test set  0.0206\n",
      "Loss on train set  0.0040217834\n",
      "Error on test set  0.020499999\n",
      "Loss on train set  0.0057330397\n",
      "Error on test set  0.02\n",
      "Loss on train set  0.014088111\n",
      "Error on test set  0.019299999\n",
      "Loss on train set  0.0029082147\n",
      "Error on test set  0.0191\n",
      "Loss on train set  0.0037059009\n",
      "Error on test set  0.0188\n",
      "Loss on train set  0.0002816235\n",
      "Error on test set  0.018299999\n",
      "Loss on train set  0.0023921556\n",
      "Error on test set  0.018599998\n",
      "Loss on train set  0.0006567788\n",
      "Error on test set  0.0187\n",
      "Loss on train set  0.010038025\n",
      "Error on test set  0.0183\n",
      "Loss on train set  0.0124664605\n",
      "Error on test set  0.0183\n",
      "Loss on train set  0.010394485\n",
      "Error on test set  0.0184\n",
      "Loss on train set  0.019597322\n",
      "Error on test set  0.0185\n",
      "Loss on train set  0.008454038\n",
      "Error on test set  0.018399999\n",
      "Loss on train set  0.0028718289\n",
      "Error on test set  0.018099999\n",
      "Loss on train set  0.012091136\n",
      "Error on test set  0.0183\n",
      "Loss on train set  0.0034028061\n",
      "Error on test set  0.0187\n",
      "Loss on train set  0.00075971533\n",
      "Error on test set  0.018900001\n",
      "Loss on train set  0.000535402\n",
      "Error on test set  0.0187\n",
      "Loss on train set  0.0010694329\n",
      "Error on test set  0.0188\n",
      "Loss on train set  0.0032487495\n",
      "Error on test set  0.018900001\n"
     ]
    }
   ],
   "source": [
    "# Improve model using convolutions\n",
    "\n",
    "def net_fn(images):\n",
    "    # LNET 300 100 10\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Conv2D(output_channels=32, kernel_shape=3),\n",
    "        hk.MaxPool(window_shape=(2,2), padding='SAME', strides=1),\n",
    "        hk.Flatten(),\n",
    "        hk.Linear(300),\n",
    "        hk.Linear(200),\n",
    "        hk.Linear(10)])\n",
    "    return mlp(images)\n",
    "net_fn_t = hk.without_apply_rng(hk.transform(net_fn))\n",
    "optimize(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2, 0, 4, 8, 7, 6, 0, 6, 3, 1, 6, 0, 7, 9, 8, 4, 5, 3, 9, 0,\n",
       "             6, 6, 3, 0, 2, 3, 6, 6, 7, 4, 0, 3, 8, 9, 5, 4, 2, 8, 5, 8,\n",
       "             5, 2, 9, 2, 4, 2, 9, 0, 5, 1, 0, 7, 9, 9, 9, 6, 3, 8, 8, 6,\n",
       "             9, 0, 5, 4], dtype=int32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
